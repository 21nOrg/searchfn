# Spec 2: Batched Persistence for Bulk Indexing

## Meta
- **Spec ID:** 2
- **Version:** 0
- **Status:** pending
- **LOE:** complex
- **Agent:** claude-sonnet-4.5
- **Created:** 2025-11-04T15:45:00Z
- **Source:** file
- **Source Ref:** docs/batched-persistence-spec.md

## What

Add batched persistence capabilities to SearchFn to dramatically improve bulk indexing performance. Currently, `SearchFn.add()` calls `persistPostings()` after every single document, causing 20,000+ IndexedDB transactions for 10,000 documents. This spec introduces three complementary mechanisms:

1. **Manual Flush API** - Explicit `flush()` method to persist accumulated changes
2. **Bulk Add API** - Convenience `addBulk()` method for batch operations  
3. **Persistence Options** - Control when persistence happens via options

## Why

### Problem
The current implementation in search-engine.ts (line 149) writes to IndexedDB after every document:

```typescript
async add(input: AddDocumentInput): Promise<void> {
  // ... process document ...
  await this.persistPostings(); // ⚠️ IndexedDB write PER document
  
  if (input.store) {
    await this.storage.putDocument({...}); // ⚠️ Another IndexedDB write
  }
}
```

**Performance Impact:**
- Indexing 10,000 documents = 20,000+ IndexedDB transactions
- Each transaction: serialization + encoding + disk I/O
- Tidigit worker reports 30-60 seconds for large datasets (device-dependent)

### Business Value
- **80%+ performance improvement** - Bulk indexing drops from 30-60s to 1-2s
- **Better developer experience** - Explicit control over persistence timing
- **Backward compatible** - Existing code continues to work
- **Unblocks tidigit** - Critical performance bottleneck resolved

## How

### High-Level Approach

**1. Core Flush Mechanism**
- Add `flush()` method to persist all pending changes
- Add `persist` option to `add()` method (default: true for backward compat)
- Queue pending documents separately from postings
- Batch document writes into single transaction

**2. Convenience API**
- Add `addBulk()` method that internally uses `{ persist: false }` + `flush()`
- Support progress callbacks for UI updates
- Handle batching automatically

**3. Backward Compatibility**
- Export `SearchEngine` as alias for `SearchFn`
- Export type aliases for all related interfaces
- Default behavior unchanged (`persist: true`)

### Key Design Decisions

**Why not single transaction for all postings?**
- Each term has different payload size (variable posting lists)
- Delta-varint encoding is term-specific
- Individual chunks enable partial loading during search
- Solution: Batch multiple term writes in one transaction using `Promise.all()`

**Why separate `pendingDocuments` queue?**
- Document storage is independent from search indexing
- Documents → `documents` store
- Postings → `terms` store
- Both can be batched separately for max efficiency

**Why `persist: true` as default?**
- Backward compatibility - existing code works without changes
- Performance-conscious users opt into manual flushing

## Breakdown

### Phase 1: Core Flush Mechanism (8-12 hours)

#### Task 1.1: Add `flush()` method to SearchFn
**File:** `src/search-engine.ts`

Add method that persists:
- Dirty postings (call existing `persistPostings()`)
- Pending documents (new batch method)
- Document stats (new helper)
- Vocabulary (new helper)

```typescript
async flush(): Promise<void> {
  await this.ensureOpen();
  
  if (this.dirtyPostings.size === 0 && 
      this.pendingDocuments.size === 0 && 
      !this.vocabularyDirty) {
    return; // Nothing to persist
  }
  
  await this.persistPostings();
  await this.batchPersistDocuments();
  await this.persistStats();
  
  if (this.vocabularyDirty) {
    await this.persistVocabulary();
    this.vocabularyDirty = false;
  }
}
```

#### Task 1.2: Add persistence option to `add()` method
**File:** `src/search-engine.ts` + `src/search-engine/types.ts`

1. Create `AddDocumentOptions` interface:
```typescript
export interface AddDocumentOptions {
  persist?: boolean; // Default: true
}
```

2. Update `add()` signature:
```typescript
async add(
  input: AddDocumentInput, 
  options?: AddDocumentOptions
): Promise<void>
```

3. Modify method body:
- Only call `persistPostings()` if `options?.persist !== false`
- For documents with `store` field:
  - If `persist: true` → immediate `putDocument()`
  - If `persist: false` → add to `pendingDocuments` queue

#### Task 1.3: Add pending documents queue
**File:** `src/search-engine.ts`

1. Add private property:
```typescript
private readonly pendingDocuments = new Map<string, Record<string, unknown>>();
```

2. Implement batch persist method:
```typescript
private async batchPersistDocuments(): Promise<void> {
  if (this.pendingDocuments.size === 0) return;
  
  const db = this.storage.assertDb();
  const tx = db.transaction([STORE_NAMES.documents], "readwrite");
  const store = tx.objectStore(STORE_NAMES.documents);
  
  const putPromises: Promise<void>[] = [];
  for (const [docId, payload] of this.pendingDocuments.entries()) {
    putPromises.push(
      new Promise((resolve, reject) => {
        const request = store.put({
          docId,
          payload,
          updatedAt: Date.now()
        });
        request.onsuccess = () => resolve();
        request.onerror = () => reject(request.error);
      })
    );
  }
  
  await Promise.all(putPromises);
  this.pendingDocuments.clear();
}
```

#### Task 1.4: Add stats and vocabulary persistence helpers
**File:** `src/search-engine.ts`

Implement four new private methods:
- `persistStats()` - Serialize and save document stats
- `loadStats()` - Deserialize and restore document stats
- `persistVocabulary()` - Save vocabulary set as JSON
- `loadVocabulary()` - Restore vocabulary set

Use `storage.putCacheState()` / `getCacheState()` with:
- Stats key: `"document-stats"`
- Vocabulary key: `"vocabulary"`

Format: JSON → TextEncoder → ArrayBuffer

#### Task 1.5: Update initialization to load stats/vocabulary
**File:** `src/search-engine.ts`

In `ensureOpen()` or `open()` method:
- Call `loadStats()` after storage is ready
- Call `loadVocabulary()` after storage is ready
- Ensure idempotency (only load once)

### Phase 2: Bulk Add API (4-6 hours)

#### Task 2.1: Add `addBulk()` method
**File:** `src/search-engine.ts` + `src/search-engine/types.ts`

1. Create interface:
```typescript
export interface BulkAddOptions {
  batchSize?: number; // Default: 1000
  onProgress?: (indexed: number, total: number) => void;
}
```

2. Implement method:
```typescript
async addBulk(
  documents: AddDocumentInput[],
  options?: BulkAddOptions
): Promise<void> {
  await this.ensureOpen();
  
  const batchSize = options?.batchSize ?? 1000;
  let indexed = 0;
  
  for (const doc of documents) {
    await this.add(doc, { persist: false });
    indexed++;
    
    if (options?.onProgress && indexed % batchSize === 0) {
      options.onProgress(indexed, documents.length);
    }
  }
  
  await this.flush();
  
  if (options?.onProgress) {
    options.onProgress(indexed, documents.length);
  }
}
```

#### Task 2.2: Export types from index
**File:** `src/index.ts`

Add exports:
```typescript
export type { AddDocumentOptions, BulkAddOptions } from "./search-engine/types";
```

### Phase 3: Backward Compatibility (2-3 hours)

#### Task 3.1: Add SearchEngine alias exports
**File:** `src/index.ts`

```typescript
// Backward compatibility: SearchEngine is now SearchFn
export { SearchFn as SearchEngine } from "./search-engine";
export type {
  SearchFnOptions as SearchEngineOptions,
  AddDocumentInput as SearchEngineAddInput,
  SearchOptions as SearchEngineSearchOptions,
  SearchResultItem as SearchEngineResultItem
} from "./search-engine/types";
```

#### Task 3.2: Update README with migration guide
**File:** `README.md`

Add section explaining:
- Main class renamed `SearchEngine` → `SearchFn`
- Backward-compatible exports available
- New batching APIs (`flush()`, `addBulk()`)
- Migration examples

### Phase 4: Testing & Documentation (6-10 hours)

#### Task 4.1: Unit tests for flush mechanism
**File:** `__tests__/batched-persistence.test.ts` (new file)

Test cases:
1. `add()` with `{ persist: false }` doesn't write to IndexedDB
2. `flush()` persists all pending changes
3. Multiple `flush()` calls are idempotent (no double-writes)
4. Stats persist and restore correctly
5. Vocabulary persists and restores correctly
6. Pending documents batch correctly

#### Task 4.2: Unit tests for bulk API
**File:** `__tests__/batched-persistence.test.ts`

Test cases:
1. `addBulk()` produces same results as individual `add()` calls
2. Progress callbacks fire at correct intervals
3. Batch size parameter works correctly
4. Search works correctly after bulk indexing

#### Task 4.3: Integration tests
**File:** `__tests__/bulk-indexing-integration.test.ts` (new file)

Test scenarios:
1. Large dataset indexing (10,000+ documents)
2. Worker snapshot export/import after bulk add
3. Search accuracy after batched indexing
4. Mixed operations (some persist, some batched)

#### Task 4.4: Performance benchmarks
**File:** `benchmarks/batched-indexing-benchmark.ts` (new file)

Benchmarks:
1. Compare old vs new implementation
2. Measure IndexedDB transaction count
3. Validate memory usage stays reasonable
4. Test with various dataset sizes (1k, 10k, 100k docs)

Create markdown report with results

#### Task 4.5: Add code example
**File:** `examples/bulk-indexing-worker.ts` (new file)

Working example showing:
- Worker-based bulk indexing
- Progress reporting
- Both manual flush and addBulk patterns
- Comparison with old approach

#### Task 4.6: Update API documentation
**Files:** `README.md`, potentially JSDoc comments

Document:
- New `flush()` method with usage examples
- New `addBulk()` method with options
- `AddDocumentOptions` interface
- Performance recommendations
- Migration path from immediate persistence

## Success Criteria

### Performance
- ✅ Bulk indexing 10,000 documents takes < 5 seconds (vs 30-60s currently)
- ✅ Memory usage stays under 100MB during bulk operations
- ✅ IndexedDB transaction count reduced by 95%+ for bulk operations

### Functionality
- ✅ All existing tests pass (backward compatibility maintained)
- ✅ `flush()` correctly persists all pending changes
- ✅ `addBulk()` produces identical search results to individual adds
- ✅ Progress callbacks work correctly
- ✅ Stats and vocabulary persist/restore correctly

### Integration
- ✅ Tidigit worker indexing time reduced by 80%+
- ✅ Example code runs successfully
- ✅ Zero breaking changes to existing API

### Documentation
- ✅ README updated with new APIs and examples
- ✅ Migration guide provided
- ✅ Code example demonstrates best practices
- ✅ Performance benchmarks documented

## Dependencies

### Internal
- **search-engine** - Core implementation target
- **storage-indexeddb** - Batch write optimization
- **indexing-indexer** - No changes, but dependency
- **query-stats** - Stats persistence integration

### External
- None - Pure optimization of existing functionality

### Blockers
- None identified

## Testing Strategy

### Unit Tests
- Test each new method in isolation
- Mock IndexedDB for controlled testing
- Verify state transitions (dirty → clean)

### Integration Tests  
- Test with real IndexedDB
- Large dataset scenarios
- Worker context testing

### Performance Tests
- Benchmark before/after
- Memory profiling
- Transaction count validation

### Regression Tests
- All existing tests must pass
- Verify backward compatibility
- Test mixed usage patterns

## Files Modified

### Core Implementation
1. `src/search-engine.ts` - Add flush(), modify add(), new helpers
2. `src/search-engine/types.ts` - New interfaces for options
3. `src/index.ts` - Backward compat exports

### Tests
4. `__tests__/batched-persistence.test.ts` - New unit tests
5. `__tests__/bulk-indexing-integration.test.ts` - New integration tests
6. `__tests__/search-engine.test.ts` - Update existing tests

### Documentation & Examples
7. `README.md` - API docs and migration guide
8. `examples/bulk-indexing-worker.ts` - New example
9. `benchmarks/batched-indexing-benchmark.ts` - New benchmark

### Configuration
10. `package.json` - Version bump when released

## Migration Path

### For Tidigit Worker

**Current implementation** (dexie.indexing.worker.ts):
```typescript
const searchEngine = new SearchEngine({...});
await searchEngine.clear();

for (let i = 0; i < records.length; i += batchSize) {
  const batch = records.slice(i, Math.min(i + batchSize, records.length));
  const addPromises: Promise<unknown>[] = [];

  for (const record of batch) {
    const fields = extractSearchFields(record, table.searchIndices);
    if (!record?.id || !fields) continue;
    addPromises.push(Promise.resolve(searchEngine.add({ id: record.id, fields })));
  }

  await Promise.allSettled(addPromises);
}
```

**Option 1: Manual Flush** (minimal changes):
```typescript
const searchEngine = new SearchEngine({...}); // Backward compat works
await searchEngine.clear();

for (let i = 0; i < records.length; i += batchSize) {
  const batch = records.slice(i, Math.min(i + batchSize, records.length));

  for (const record of batch) {
    const fields = extractSearchFields(record, table.searchIndices);
    if (!record?.id || !fields) continue;
    await searchEngine.add({ id: record.id, fields }, { persist: false });
  }

  // Progress reporting...
}

await searchEngine.flush(); // Single flush at end
```

**Option 2: Bulk API** (recommended):
```typescript
const searchEngine = new SearchEngine({...});
await searchEngine.clear();

const documents = records
  .map(r => {
    const fields = extractSearchFields(r, table.searchIndices);
    if (!r?.id || !fields) return null;
    return { id: r.id, fields };
  })
  .filter(Boolean);

await searchEngine.addBulk(documents, {
  batchSize: 1000,
  onProgress: (indexed, total) => {
    currentProgress.indexedRecords = indexed;
    currentProgress.progress = Math.floor((indexed / total) * 100);
    broadcastProgress(port);
  }
});
```

## Rollout Plan

### Week 1: Core Implementation
- Phase 1 tasks (flush mechanism)
- Basic unit tests
- Internal validation

### Week 2: Bulk API & Compatibility
- Phase 2 tasks (addBulk)
- Phase 3 tasks (backward compat)
- Integration tests

### Week 3: Testing & Documentation
- Phase 4 tasks
- Performance benchmarks
- Examples and docs

### Week 4: Integration & Validation
- Migrate tidigit worker
- Validate performance gains
- User acceptance testing

## Future Enhancements

These are OUT OF SCOPE for this spec but noted for future consideration:

1. **Auto-throttled persistence** - Automatically flush after N seconds or M documents
2. **Memory pressure detection** - Auto-flush when memory usage exceeds threshold  
3. **Streaming indexing** - Support datasets larger than memory
4. **Progress events** - EventEmitter interface for fine-grained tracking
5. **Transaction batching** - Group multiple term writes into fewer transactions

## Notes

- **Performance target is aggressive but achievable** - 15-30x speedup from eliminating redundant I/O
- **Backward compatibility is critical** - Existing code must work without changes
- **Memory management is important** - Monitor heap usage during development
- **Testing strategy is comprehensive** - Unit, integration, performance, and regression tests
- **Tidigit integration is the primary driver** - Keep their use case in mind throughout

## Related Features

- **search-engine** - Core target for implementation
- **storage-indexeddb** - Storage layer being optimized
- **indexing-indexer** - Indexing pipeline that feeds into add()
- **query-stats** - Stats manager that needs persistence integration

## Risk Assessment

**Low Risk:**
- Backward compatibility maintained by default behavior
- Changes are additive (new methods, optional parameters)
- Existing test suite catches regressions

**Medium Risk:**
- Memory usage during bulk operations needs monitoring
- IndexedDB transaction batching could reveal browser-specific bugs
- Performance improvements need validation across browsers

**Mitigation:**
- Comprehensive testing including memory profiling
- Cross-browser testing in CI
- Incremental rollout with feature flags if needed
