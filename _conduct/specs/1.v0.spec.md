# Spec 1: Fuzzy Matching, Autocomplete, and Partial Search

## Meta
- **Spec ID:** 1
- **Version:** 0
- **Status:** pending
- **LOE:** complex
- **Agent:** claude-sonnet-4.5
- **Created:** 2025-11-04T15:30:00Z
- **Source:** file
- **Source Ref:** docs/fuzzy-autocomplete-plan.md

## What

Implement three complementary search capabilities for SearchFn and InMemorySearchFn:

1. **Partial/Prefix Search**: Match incomplete terms (e.g., "an" matches "anthropic")
2. **Autocomplete**: Real-time suggestions as users type
3. **Fuzzy Matching**: Handle typos and spelling variations (e.g., "anthopric" matches "anthropic")

Currently, the search engine only matches complete tokens after tokenization, limiting usability for interactive search experiences.

## Why

**Problem**: Users need to type complete words to get matches, which creates friction in search interfaces:
- No autocomplete support for dropdown suggestions
- Typos result in zero results instead of "did you mean" suggestions
- Prefix matching doesn't work, forcing users to remember exact terms

**Value**: 
- Improves user experience with real-time search suggestions
- Increases search success rate by tolerating typos
- Enables modern autocomplete UIs
- Reduces cognitive load on users

## How

**Three-Phase Implementation** (~3 weeks):

### Phase 1: Edge N-Grams for Prefix Search (Week 1)
- Implement EdgeNGramStage pipeline stage that generates all prefixes at index time
- Example: "anthropic" generates ["an", "ant", "anth", "anthr", "anthro", "anthrop", "anthropi", "anthropic"]
- Trade-off: 2-3x index size increase for fast query-time prefix lookup
- Add configurable options: `enableEdgeNGrams`, `edgeNGramMinLength`, `edgeNGramMaxLength`

### Phase 2: Fuzzy Matching via Edit Distance (Week 2)
- Implement Levenshtein distance algorithm for computing edit distance
- Add vocabulary tracking to maintain set of all indexed terms
- Implement query-time expansion: expand query terms to similar terms within max edit distance
- Add fuzzy search option to SearchOptions: `fuzzy?: number | boolean`
- Trade-off: No index overhead, but slower query performance for fuzzy searches

### Phase 3: Optimization & Hybrid Strategy (Week 3)
- Implement auto mode detection: short queries use prefix, long queries use fuzzy, medium use exact
- Add LRU cache for fuzzy expansions to avoid recomputation
- Optional: Implement BK-Tree for efficient fuzzy lookup on large vocabularies (>50k terms)
- Add per-field configuration for selective n-gram application

**Key Technical Decisions**:
- Edge n-grams over radix tree: simpler implementation, works with existing inverted index
- Query-time expansion over index-time n-grams for fuzzy: no index bloat for rare use case
- Hybrid approach: optimize for common cases (short=prefix, long=fuzzy)

## Breakdown

### Phase 1: Prefix Search (Priority: High, ~5 days)

**1.1 Core Pipeline Stage** (1 day)
- Create `src/pipeline/stages/edge-ngram-stage.ts`
- Implement EdgeNGramStage with configurable min/max gram length
- Handle token metadata to track original term and prefix flag
- Support configurable min/max lengths (default: 2-15)

**1.2 Pipeline Integration** (1 day)
- Add `enableEdgeNGrams`, `edgeNGramMinLength`, `edgeNGramMaxLength` to PipelineOptions
- Integrate EdgeNGramStage into pipeline builder
- Support per-field configuration via `fieldConfig`

**1.3 SearchFn Integration** (1 day)
- Update SearchFn to use edge n-grams when enabled
- Add `prefixMatch?: boolean` to SearchOptions
- **IndexedDB persistence details**:
  - N-gram tokens stored in existing inverted index structure
  - Posting key format: `${field}:${ngram_term}` (same as regular terms)
  - Metadata stored in posting: `{ isPrefix: boolean, originalTerm: string }`
  - No schema changes required - n-grams are just additional terms
  - Chunk serialization handles n-gram metadata transparently

**1.4 InMemorySearchFn Integration** (1 day)
- Update InMemorySearchFn to use edge n-grams when enabled
- **Worker snapshot serialization details**:
  - N-gram metadata included in token serialization
  - Snapshot format: `{ term: string, metadata: { isPrefix: boolean, originalTerm: string } }`
  - Vocabulary set serialized as array for worker transfer
  - Transfer format uses structured clone algorithm (supports Map/Set)

**1.5 Scoring Adjustments** (1 day)
- Apply prefix match penalty (0.7x) vs exact matches
- Update BM25 scoring to handle n-gram metadata
- Test scoring with prefix vs exact match combinations

**1.6 Tests & Documentation**
- Unit tests: n-gram generation, scoring penalties
- Integration tests: end-to-end prefix search
- Add autocomplete example to examples/
- Update README with prefix search configuration

### Phase 2: Fuzzy Matching (Priority: Medium, ~5 days)

**2.1 Levenshtein Distance** (1 day)
- Create `src/utils/levenshtein.ts`
- Implement levenshteinDistance function with dynamic programming
- Implement fuzzyExpand function with length pre-filtering
- Optimize with early exit conditions

**2.2 Vocabulary Tracking** (1 day)
- Add `private vocabulary = new Set<string>()` to SearchFn/InMemorySearchFn
- Track unique terms during document indexing (original terms only, not n-grams)
- **Persistence implementation**:
  - SearchFn: Store vocabulary as special IndexedDB record with key `__vocabulary__`
  - Serialize as JSON array: `JSON.stringify(Array.from(vocabulary))`
  - Load on mount: deserialize back to Set
  - Update incrementally on each document add/remove
- **Worker snapshot implementation**:
  - InMemorySearchFn: Include vocabulary in snapshot object
  - Format: `{ index: {...}, vocabulary: Array.from(vocabulary) }`
  - Reconstruct Set from array after transfer: `new Set(snapshot.vocabulary)`

**2.3 Query Expansion** (1 day)
- Implement buildQueryTokens with fuzzy expansion logic
- Expand query terms within max edit distance
- Apply boost penalty (0.8x) for fuzzy matches vs exact matches
- Handle edge cases: very short terms, empty expansions

**2.4 Search API** (1 day)
- Add `fuzzy?: number | boolean` to SearchOptions
- Default fuzzy distance to 2 when `fuzzy: true`
- Update search method to use fuzzy expansion when enabled
- Ensure backward compatibility (fuzzy off by default)

**2.5 Tests & Documentation** (1 day)
- Unit tests: Levenshtein distance, fuzzy expansion
- Integration tests: end-to-end fuzzy search, typo tolerance
- Add fuzzy search example to examples/
- Document performance characteristics

### Phase 3: Optimization (Priority: Low, ~4 days)

**3.1 Auto Mode Detection** (1 day)
- Implement determineMode logic: short→prefix, long→fuzzy, medium→exact
- Add `mode?: 'exact' | 'prefix' | 'fuzzy' | 'auto'` to SearchOptions
- Tune thresholds based on benchmarks (≤3: prefix, ≥8: fuzzy)

**3.2 Fuzzy Caching** (1 day)
- Implement LRU cache for fuzzy expansions
- Cache key: `${term}:${maxDistance}`
- Default: 1000 entries
- Add cache hit/miss metrics

**3.3 BK-Tree (Optional)** (2 days)
- Implement BK-Tree data structure for efficient fuzzy lookup
- Only activate when vocabulary size > 50k terms
- Benchmark: compare linear scan vs BK-tree lookup
- Document when to enable BK-tree optimization

**3.4 Per-Field Config** (1 day)
- Support `fieldConfig` in pipeline options
- Example: n-grams only on title field, not body
- Add configuration guide to docs

### Testing & Quality

**Unit Tests**
- Edge n-gram generation (various lengths)
- Levenshtein distance (edge cases: empty strings, long strings)
- Fuzzy expansion (length filtering, distance limits)
- Vocabulary tracking (add, serialize, deserialize)
- Scoring adjustments (prefix penalty, fuzzy penalty)

**Integration Tests**
- Prefix search end-to-end (SearchFn + InMemorySearchFn)
- Fuzzy search end-to-end with real typos
- Hybrid mode selection based on query length
- IndexedDB persistence with n-grams
- Worker snapshot serialization

**Performance Tests**
- Index size benchmarks (baseline vs n-grams)
- Query latency benchmarks (exact, prefix, fuzzy)
- Memory usage profiling (in-memory with/without n-grams)
- Fuzzy expansion timing (linear vs BK-tree)
- Indexing speed benchmarks

**Benchmark Goals**:
- Prefix search: <5ms latency for 10k documents
- Fuzzy search: <50ms latency for edit distance 2
- Index size: <3x increase with n-grams enabled
- Memory: <2x increase with n-grams enabled

### Migration & Backward Compatibility

**Considerations**:
- Existing indexes don't have n-grams → progressive enhancement
- Default: `enableEdgeNGrams: false` for backward compatibility
- Support opt-in per query: `prefixMatch: true` even without indexed n-grams (degraded experience)
- Provide re-indexing utilities for upgrading existing indexes

**Re-indexing Path**:
```typescript
// Check if index has n-gram support
const hasNGrams = await engine.hasFeature('edgeNGrams');
if (!hasNGrams) {
  await engine.rebuildIndex(documents, {
    pipeline: { enableEdgeNGrams: true }
  });
}
```

## Success Criteria

**Functional**:
- [ ] Prefix search: "an" matches "anthropic"
- [ ] Fuzzy search: "anthopric" matches "anthropic" (edit distance ≤ 2)
- [ ] Autocomplete works with real-time suggestions
- [ ] Scoring: exact matches rank higher than prefix/fuzzy matches
- [ ] Configuration: per-field n-gram settings work correctly
- [ ] Persistence: n-grams and vocabulary survive IndexedDB round-trip

**Non-Functional**:
- [ ] Index size: ≤3x increase with n-grams enabled
- [ ] Query latency: prefix search <5ms, fuzzy search <50ms (10k docs)
- [ ] Memory: ≤2x increase with n-grams
- [ ] Backward compatible: existing code works without changes
- [ ] Test coverage: >90% for new code

**Documentation**:
- [ ] Configuration guide with examples (autocomplete, fuzzy, hybrid)
- [ ] Performance considerations documented (index size, query speed)
- [ ] Migration guide for existing indexes
- [ ] Examples: autocomplete UI, typo-tolerant search

## Dependencies

**Internal**:
- Pipeline system (`src/pipeline/`)
- Indexing system (`src/search-engine/indexer.ts`)
- Query engine (`src/query/`)
- Storage layer (`src/storage/`)

**External**:
- None (pure TypeScript implementation)

**Blockers**:
- None identified

**Nice-to-Have**:
- BK-Tree library (could implement from scratch or use existing)
- Benchmark suite for performance validation

## Linked Features

**Primary Features**:
- `searchfn:pipeline` - Text processing pipeline (will add EdgeNGramStage)
- `searchfn:pipeline-tokenization` - Tokenization stages (n-gram is a tokenization variant)
- `searchfn:query-engine` - Query engine (fuzzy expansion logic)
- `searchfn:search-engine` - Search engine core (integration point)

**Secondary Features**:
- `searchfn:indexing-indexer` - Main indexer (vocabulary tracking)
- `searchfn:query-scoring` - BM25 scoring (prefix/fuzzy penalties)
- `searchfn:storage-indexeddb` - IndexedDB persistence (n-gram storage)

## Notes

**Trade-offs**:
- Edge n-grams: 2-3x index size for fast prefix search (acceptable for most use cases)
- Fuzzy matching: Slower queries but no index overhead (good for occasional use)
- Hybrid approach balances both concerns

**Alternatives Considered**:
- ❌ Radix tree: Too complex, requires major refactoring
- ❌ Full n-grams: Explosive index growth, not worth it
- ❌ Soundex: Limited utility for technical terms

**Future Enhancements**:
- Phonetic matching (Soundex, Metaphone) for name searches
- Context/phrase search (term proximity tracking)
- Contextual autocomplete with query history
- Distributed fuzzy search for very large corpora

**Reference**: Full technical details in `docs/fuzzy-autocomplete-plan.md`
