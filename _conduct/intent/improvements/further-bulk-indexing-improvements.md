
> generated by zed (v0 API)
Based on my analysis of the codebase, I can see how bulk indexing is currently implemented and identify several areas for improvement. Let me provide a comprehensive analysis and propose optimizations.

## Current Bulk Indexing Implementation

The current implementation uses a three-tier approach:

```searchfn/src/search-engine.ts#L228-265
async addBulk(documents: AddDocumentInput[], options?: BulkAddOptions): Promise<void> {
  // Process in batches of 100 (default)
  // Each document calls add() with persist: false
  // Uses Promise.all() for parallel processing within batches
  // Single flush() at the end
}
```

**Key characteristics:**
- **Batch size**: 100 documents (configurable)
- **Processing**: Parallel within batches using `Promise.all()`
- **Persistence**: Deferred until `flush()`
- **Search availability**: Immediate (caches updated per document)
- **Performance**: 15-30x faster than individual persistence

## Identified Improvement Opportunities

### 1. **Document Persistence Batching** (High Impact)

**Current Issue**: Documents are persisted individually in parallel, not in batched transactions.

```searchfn/src/search-engine.ts#L554-570
private async batchPersistDocuments(): Promise<void> {
  const putPromises: Promise<void>[] = [];
  for (const [docId, payload] of this.pendingDocuments.entries()) {
    putPromises.push(this.storage.putDocument({ docId, payload, updatedAt: Date.now() }));
  }
  await Promise.all(putPromises); // Individual transactions!
}
```

**Proposed Solution**: Add `putDocumentsBatch()` method similar to `putTermChunksBatch()`:

```searchfn/src/storage/indexeddb-manager.ts#L200-220
async putDocumentsBatch(documents: StoredDocumentRecord[]): Promise<void> {
  if (documents.length === 0) return;

  await this.withTransaction([STORE_NAMES.documents], "readwrite", async (tx) => {
    const store = tx.objectStore(STORE_NAMES.documents);
    const putPromises = documents.map(record => {
      const dbRecord: DocumentDbRecord = {
        docId: normaliseDocId(record.docId),
        payload: record.payload,
        updatedAt: record.updatedAt
      };
      return this.requestToPromise(store.put(dbRecord));
    });
    await Promise.all(putPromises);
  });
}
```

### 2. **Adaptive Memory Management** (Medium Impact)

**Current Issue**: Fixed batch size regardless of document size or memory pressure.

**Proposed Solution**: Dynamic batch sizing based on memory usage:

```searchfn/src/search-engine.ts#L270-300
interface AdaptiveBatchingOptions extends BulkAddOptions {
  maxMemoryMB?: number; // Default: 50MB
  minBatchSize?: number; // Default: 10
  maxBatchSize?: number; // Default: 1000
}

private calculateAdaptiveBatchSize(
  documents: AddDocumentInput[],
  startIndex: number,
  options: AdaptiveBatchingOptions
): number {
  const maxMemory = (options.maxMemoryMB ?? 50) * 1024 * 1024;
  const minBatch = options.minBatchSize ?? 10;
  const maxBatch = options.maxBatchSize ?? 1000;

  let batchSize = 0;
  let estimatedMemory = 0;

  for (let i = startIndex; i < documents.length && batchSize < maxBatch; i++) {
    const doc = documents[i];
    const docSize = JSON.stringify(doc).length * 2; // Rough UTF-16 estimate

    if (estimatedMemory + docSize > maxMemory && batchSize >= minBatch) {
      break;
    }

    estimatedMemory += docSize;
    batchSize++;
  }

  return Math.max(batchSize, minBatch);
}
```

### 3. **Bulk Pipeline Optimization** (High Impact)

**Current Issue**: Each document processed individually through the indexing pipeline.

**Proposed Solution**: Batch-optimized indexer:

```searchfn/src/indexing/bulk-indexer.ts#L1-50
export class BulkIndexer extends Indexer {
  ingestBatch(records: IndexingInputRecord[]): IngestedDocument[] {
    // Pre-allocate maps for better memory efficiency
    const results: IngestedDocument[] = [];
    const fieldTokenCache = new Map<string, Map<string, Token[]>>();

    // Batch tokenization by field to leverage pipeline optimizations
    for (const record of records) {
      for (const [field, text] of Object.entries(record.fields)) {
        if (!text) continue;

        let fieldCache = fieldTokenCache.get(field);
        if (!fieldCache) {
          fieldCache = new Map();
          fieldTokenCache.set(field, fieldCache);
        }

        if (!fieldCache.has(text)) {
          fieldCache.set(text, this.pipeline.run(field, text, record.docId));
        }
      }
    }

    // Process documents using cached tokens
    for (const record of records) {
      const accumulator = new DocumentAccumulator(record.docId);

      for (const [field, text] of Object.entries(record.fields)) {
        if (!text) continue;
        const tokens = fieldTokenCache.get(field)?.get(text) ?? [];
        for (const token of tokens) {
          accumulator.addToken(token);
        }
      }

      results.push(this.convertToIngestedDocument(accumulator));
    }

    return results;
  }
}
```

### 4. **Optimized Cache Update Strategy** (Medium Impact)

**Current Issue**: Cache updates after every document can be expensive.

**Proposed Solution**: Batch cache updates within processing batches:

```searchfn/src/search-engine.ts#L350-380
private updateCachesBatch(ingestedDocs: IngestedDocument[]): void {
  const termUpdates = new Map<string, Map<string, TermPosting[]>>();

  // Collect all updates first
  for (const doc of ingestedDocs) {
    for (const [field, termFreqs] of doc.fieldFrequencies.entries()) {
      const metadata = doc.fieldMetadata.get(field) ?? new Map();

      for (const [term, frequency] of termFreqs.entries()) {
        const key = this.getPostingKey(field, term);

        if (!termUpdates.has(key)) {
          termUpdates.set(key, new Map());
        }

        const termMap = termUpdates.get(key)!;
        if (!termMap.has(field)) {
          termMap.set(field, []);
        }

        termMap.get(field)!.push({
          docId: doc.docId,
          termFrequency: frequency,
          metadata: metadata.get(term)
        });
      }
    }
  }

  // Apply all updates in batch
  for (const [key, fieldMap] of termUpdates.entries()) {
    // Update postings and caches efficiently
    this.applyBatchedPostingUpdates(key, fieldMap);
  }
}
```

### 5. **Streaming Progress Reporting** (Low Impact)

**Current Issue**: Progress only reported at batch boundaries.

**Proposed Solution**: Intra-batch progress reporting:

```searchfn/src/search-engine.ts#L400-430
async addBulkStreaming(
  documents: AddDocumentInput[],
  options?: BulkAddOptions & { streamingProgress?: boolean }
): Promise<void> {
  // ... existing setup ...

  for (let i = 0; i < documents.length; i += batchSize) {
    const batch = documents.slice(i, Math.min(i + batchSize, documents.length));

    if (options?.streamingProgress && options?.onProgress) {
      // Process documents one by one with progress updates
      for (let j = 0; j < batch.length; j++) {
        await this.add(batch[j], { persist: false });
        const currentIndex = i + j + 1;

        if (currentIndex % 10 === 0 || currentIndex === documents.length) {
          options.onProgress(currentIndex, documents.length);
        }
      }
    } else {
      // Existing batch processing
      await Promise.all(batch.map(doc => this.add(doc, { persist: false })));
    }

    indexed += batch.length;
  }
}
```

### 6. **Enhanced Error Recovery** (Medium Impact)

**Current Issue**: No partial failure recovery for large bulk operations.

**Proposed Solution**: Checkpointing system:

```searchfn/src/search-engine.ts#L450-490
interface BulkIndexingCheckpoint {
  processedCount: number;
  lastSuccessfulBatch: number;
  failedDocuments: Array<{ index: number; error: Error }>;
}

async addBulkWithRecovery(
  documents: AddDocumentInput[],
  options?: BulkAddOptions & {
    enableCheckpointing?: boolean;
    checkpointInterval?: number;
  }
): Promise<BulkIndexingCheckpoint> {
  const checkpoint: BulkIndexingCheckpoint = {
    processedCount: 0,
    lastSuccessfulBatch: -1,
    failedDocuments: []
  };

  const checkpointInterval = options?.checkpointInterval ?? 1000;

  for (let i = 0; i < documents.length; i += batchSize) {
    try {
      const batch = documents.slice(i, Math.min(i + batchSize, documents.length));
      await Promise.all(batch.map(doc => this.add(doc, { persist: false })));

      checkpoint.processedCount += batch.length;
      checkpoint.lastSuccessfulBatch = Math.floor(i / batchSize);

      // Periodic checkpointing
      if (options?.enableCheckpointing &&
          checkpoint.processedCount % checkpointInterval === 0) {
        await this.flush(); // Persist progress
      }

    } catch (error) {
      // Record failed batch and continue
      for (let j = 0; j < batch.length; j++) {
        checkpoint.failedDocuments.push({
          index: i + j,
          error: error as Error
        });
      }
    }
  }

  await this.flush();
  return checkpoint;
}
```

## Recommended Implementation Priority

1. **High Priority**: Document persistence batching and bulk pipeline optimization
2. **Medium Priority**: Adaptive memory management and enhanced error recovery
3. **Low Priority**: Streaming progress reporting

These improvements would maintain the current search performance guarantees while significantly optimizing bulk indexing throughput, especially for large datasets. The key insight is that most optimizations focus on the persistence layer and memory management, while keeping the immediate cache updates that ensure search availability.
